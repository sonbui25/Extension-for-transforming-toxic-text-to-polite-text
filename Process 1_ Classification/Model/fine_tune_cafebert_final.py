# -*- coding: utf-8 -*-
"""fine-tune-cafebert-final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1foiNWX7GaXtT7SDZJPOnT-fqpeGf1Q_D
"""

from datasets import load_dataset, Dataset
import numpy as np
import pandas as pd
import re
import emoji
import unicodedata
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

from transformers.utils import move_cache
move_cache()

"""# Preprocessing input"""

# Login using e.g. `huggingface-cli login` to access this dataset
ds = load_dataset("UngLong/ViSIR")

df_train = pd.DataFrame(ds['train'])
df_valid = pd.DataFrame(ds['validation'])
df_test = pd.DataFrame(ds['test'])

df_train.head()

# 1️⃣ Dictionary chứa từ viết tắt toxic
toxic_abbreviations = {
    "vl": "vãi lồn",
    "vkl": "vãi cả lồn",
    "dm": "đụ mẹ",
    "dmm": "đụ mẹ mày",
    "cl": "cái lồn",
    "vc": "vãi cặc",
    "đmm": "đụ mẹ mày",
    "cc": "con cặc",
    "lol": "lồn",
    "dkm": "địt con mẹ",
    "đcmm": "địt con mẹ mày",
    "wtf": "cái đéo gì vậy",
    "clmm": "cái lồn mẹ mày",
    "ccmm": "con chó mẹ mày",
    "dcm": "địt con mẹ mày",
    "m": "mày",
    "t": "tao",
    "vcl": "vãi cả lồn",
    "nguu": "ngu",
    "tk": "thằng",
    "djt": "địt",
    "lmm": "lồn mẹ mày",
    "lòn": "lồn",
    "óc c": "óc cặc",
    "clm": "cái lồn má",
    "djtme": "địt mẹ",
    "đcho": "đĩ chó",
    "iồn": "lồn"
}

# 2️⃣ Regex thay thế từ viết tắt toxic (không nhầm trong từ khác)
toxic_pattern = re.compile(r"\b(" + "|".join(map(re.escape, toxic_abbreviations.keys())) + r")(?=\W|$)", re.IGNORECASE)

# 3️⃣ Hàm mở rộng từ viết tắt
def expand_toxic_abbreviations(text):
    return toxic_pattern.sub(lambda m: toxic_abbreviations[m.group(1).lower()], text)

# 4️⃣ Hàm làm sạch văn bản
def clean_text(text):
    # Chuyển sang string nếu không phải string
    if not isinstance(text, str):
        text = str(text)

    # Chuẩn hóa Unicode
    text = unicodedata.normalize("NFC", text)

    # Loại bỏ emoji
    text = re.sub(r"[^\w\s\u00C0-\u1FFF\u2C00-\uD7FF]", " ", text)

    # Thay thế từ viết tắt toxic
    text = expand_toxic_abbreviations(text)

    # Xóa khoảng trắng thừa
    text = re.sub(r"\s+", " ", text)

    return text.lower().strip()

df_train['text'] = df_train['text'].apply(clean_text)
df_valid['text'] = df_valid['text'].apply(clean_text)
df_test['text'] = df_test['text'].apply(clean_text)

"""# Fine-tune"""

# Bước 2: Chuẩn bị tokenizer
model_checkpoint = "uitnlp/CafeBERT"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

dataset_train = Dataset.from_pandas(df_train)
dataset_valid = Dataset.from_pandas(df_valid)
dataset_test = Dataset.from_pandas(df_test)

def get_percentile_length(dataset, percentile=99):
    token_lens = [len(tokenizer.tokenize(text)) for text in dataset["text"]]
    return int(np.percentile(token_lens, percentile))

suggested_max_length = max(
    get_percentile_length(dataset_train),
    get_percentile_length(dataset_valid),
    get_percentile_length(dataset_test)
)

print(f"Suggested max_length (99th percentile): {suggested_max_length}")

# Hàm tokenization
def tokenize_function(examples):
    tokenized_inputs = tokenizer(
        examples["text"],  # Cột chứa văn bản
        padding="max_length",
        truncation=True,
        max_length=77
    )
    tokenized_inputs["labels"] = examples["label"]  # Gán label
    return tokenized_inputs

# Áp dụng tokenization cho từng tập dữ liệu
tokenized_train = dataset_train.map(tokenize_function, batched=True)
tokenized_valid = dataset_valid.map(tokenize_function, batched=True)
tokenized_test = dataset_test.map(tokenize_function, batched=True)

# Bước 3: Load mô hình CafeBERT
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)

training_args = TrainingArguments(
    output_dir="./finetuned_cafebert",  # Đổi sang thư mục khác để tránh lỗi safepoint
    eval_strategy="epoch",  # Đánh giá sau mỗi epoch
    save_strategy="epoch",  # Lưu checkpoint mỗi epoch
    save_total_limit=2,  # Chỉ giữ lại 2 checkpoint gần nhất, tránh đầy ổ cứng
    learning_rate=2e-5,
    per_device_train_batch_size=16,  # Tăng batch size nếu GPU đủ RAM
    per_device_eval_batch_size=16,
    num_train_epochs=3,  # Có thể giảm nếu dataset lớn
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,  # Giảm số lần log để tránh overload
    push_to_hub=False,  # Không đẩy lên Hugging Face
    report_to="none"  # Không log lên W&B
)

# Bước 5: Định nghĩa Trainer
# Định nghĩa Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,  # Đã được tokenized
    eval_dataset=tokenized_valid    # Đã được tokenized
)

# Bước 6: Tiến hành fine-tuning
trainer.train()

import os
os.listdir("./finetuned_cafebert")

import json

checkpoint_path = "./finetuned_cafebert/checkpoint-2410/trainer_state.json"
with open(checkpoint_path, "r") as f:
    trainer_state = json.load(f)

print(trainer_state["epoch"])

final_model = AutoModelForSequenceClassification.from_pretrained("./finetuned_cafebert/checkpoint-2410")

# Lưu model cuối cùng xuống local nếu cần
final_model.save_pretrained("./cafebert_final_model")
tokenizer.save_pretrained("./cafebert_final_model")

import shutil

shutil.make_archive("cafebert_final_model", 'zip', "./cafebert_final_model")

from IPython.display import FileLink
FileLink(r'/kaggle/working/cafebert_final_model.zip')

from google.colab import files

files.download('/kaggle/working/cafebert_final_model.zip')

from huggingface_hub import notebook_login
notebook_login()

from huggingface_hub import HfApi

repo_name = "cafebert-classification-ft"  # Đặt tên repo
api = HfApi()
repo_url = api.create_repo(repo_name, repo_type="model", exist_ok=True)

print(f"Repo created: {repo_url}")

print(repo_url)

from huggingface_hub import upload_folder

upload_folder(
    folder_path="./cafebert_final_model",  # Thư mục chứa model
    repo_id="UngLong/cafebert-classification-ft",  # Repo của bạn
    repo_type="model"
)

